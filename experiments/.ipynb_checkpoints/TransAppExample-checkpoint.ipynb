{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ©2023 EDF\n",
    "Adrien PETRALIA - EDF R&D and Université Paris Cité (LIPADE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADF & TransApp - Notebook example\n",
    "## A Transformer-Based Framework for Appliance Detection Using Smart Meter Consumption Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch-1.8.1/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "root = Path(os.getcwd()).resolve().parents[0]\n",
    "sys.path.append(str(root))\n",
    "from experiments.data_utils import *\n",
    "from src.TransAppModel.TransApp import *\n",
    "from src.AD_Framework.Framework import *\n",
    "from src.utils.losses import *\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiation of a TransApp Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_inst(m, win, dim_model, mode=\"pretraining\", large_version=False, path_select_core=None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        Get TransApp model instance\n",
    "    \n",
    "    Parameters:\n",
    "        m: int - n channel of input time series\n",
    "        win: int - length of input subsequence (usefull for positional encoding, if any)\n",
    "        mode: str - 'pretraining' or 'classif' (type of head)\n",
    "        large_version: boolean - if true, use 5 encoder layers instead of 3\n",
    "        path_select_core: str - path to pretrained instance of TransApp \n",
    "    \"\"\"\n",
    "\n",
    "    TApp = TransApp(max_len=win, c_in=m,\n",
    "                    mode=mode,\n",
    "                    n_embed_blocks=1, \n",
    "                    encoding_type='noencoding',\n",
    "                    n_encoder_layers=5 if large_version else 3,\n",
    "                    kernel_size=5,\n",
    "                    d_model=dim_model, pffn_ratio=2, n_head=4,\n",
    "                    prenorm=True, norm=\"LayerNorm\",\n",
    "                    activation='gelu',\n",
    "                    store_att=False, attn_dp_rate=0.2, head_dp_rate=0., dp_rate=0.2,\n",
    "                    att_param={'attenc_mask_diag': True, 'attenc_mask_flag': False, 'learnable_scale_enc': False},\n",
    "                    c_reconstruct=1, apply_gap=True, nb_class=2)\n",
    "\n",
    "    if path_select_core is not None:\n",
    "        TApp.load_state_dict(torch.load(path_select_core)['model_state_dict'])\n",
    "\n",
    "    return TApp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-supervised pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "TransApp                                                [1, 1, 1024]              --\n",
       "├─Sequential: 1-1                                       [1, 1024, 64]             --\n",
       "│    └─DilatedBlock: 2-1                                [1, 64, 1024]             --\n",
       "│    │    └─Sequential: 3-1                             [1, 64, 1024]             64,192\n",
       "│    └─Transpose: 2-2                                   [1, 1024, 64]             --\n",
       "├─Sequential: 1-2                                       [1, 1024, 64]             37,888\n",
       "│    └─EncoderLayer: 2-3                                [1, 1024, 64]             16,832\n",
       "│    │    └─LayerNorm: 3-4                              [1, 1024, 64]             (recursive)\n",
       "│    │    └─AttentionLayer: 3-3                         [1, 1024, 64]             16,640\n",
       "│    │    └─LayerNorm: 3-4                              [1, 1024, 64]             (recursive)\n",
       "│    │    └─LayerNorm: 3-11                             [1, 1024, 64]             (recursive)\n",
       "│    │    └─Dropout: 3-13                               [1, 1024, 64]             --\n",
       "│    │    └─PositionWiseFeedForward: 3-12               [1, 1024, 64]             (recursive)\n",
       "│    └─EncoderLayer: 2-5                                [1, 1024, 64]             (recursive)\n",
       "│    │    └─AttentionLayer: 3-15                        [1, 1024, 64]             (recursive)\n",
       "│    │    └─LayerNorm: 3-18                             [1, 1024, 64]             (recursive)\n",
       "│    │    └─Dropout: 3-20                               [1, 1024, 64]             --\n",
       "│    └─EncoderLayer: 2                                  --                        --\n",
       "│    │    └─LayerNorm: 3-11                             [1, 1024, 64]             (recursive)\n",
       "│    │    └─PositionWiseFeedForward: 3-12               [1, 1024, 64]             (recursive)\n",
       "│    │    └─Dropout: 3-13                               [1, 1024, 64]             --\n",
       "│    └─EncoderLayer: 2-5                                [1, 1024, 64]             (recursive)\n",
       "│    │    └─LayerNorm: 3-14                             [1, 1024, 64]             128\n",
       "│    │    └─AttentionLayer: 3-15                        [1, 1024, 64]             (recursive)\n",
       "│    └─EncoderLayer: 2                                  --                        --\n",
       "│    │    └─LayerNorm: 3-23                             [1, 1024, 64]             (recursive)\n",
       "│    │    └─Dropout: 3-25                               [1, 1024, 64]             --\n",
       "│    └─EncoderLayer: 2                                  --                        --\n",
       "│    │    └─LayerNorm: 3-18                             [1, 1024, 64]             (recursive)\n",
       "│    │    └─PositionWiseFeedForward: 3-19               [1, 1024, 64]             16,576\n",
       "├─Sequential: 1-3                                       --                        --\n",
       "│    └─Transpose: 2-6                                   --                        --\n",
       "│    └─AdaptiveAvgPool1d: 2-7                           --                        --\n",
       "│    └─Flatten: 2-8                                     --                        --\n",
       "│    └─Linear: 2-9                                      --                        130\n",
       "│    └─Dropout: 2-10                                    --                        --\n",
       "├─Sequential: 1                                         --                        --\n",
       "│    └─EncoderLayer: 2                                  --                        --\n",
       "│    │    └─Dropout: 3-20                               [1, 1024, 64]             --\n",
       "│    └─EncoderLayer: 2-11                               [1, 1024, 64]             128\n",
       "│    │    └─LayerNorm: 3-21                             [1, 1024, 64]             128\n",
       "│    │    └─AttentionLayer: 3-22                        [1, 1024, 64]             16,640\n",
       "│    │    └─LayerNorm: 3-23                             [1, 1024, 64]             (recursive)\n",
       "│    │    └─PositionWiseFeedForward: 3-24               [1, 1024, 64]             16,576\n",
       "│    │    └─Dropout: 3-25                               [1, 1024, 64]             --\n",
       "│    └─LayerNorm: 2-12                                  [1, 1024, 64]             128\n",
       "├─Sequential: 1-4                                       [1, 1024, 1]              --\n",
       "│    └─Linear: 2-13                                     [1, 1024, 1]              65\n",
       "│    └─Dropout: 2-14                                    [1, 1024, 1]              --\n",
       "=========================================================================================================\n",
       "Total params: 164,931\n",
       "Trainable params: 164,931\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 65.74\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 13.12\n",
       "Params size (MB): 0.52\n",
       "Estimated Total Size (MB): 13.66\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m       = 5    # Number of channel of the input time series (i.e. consumption time series, hours encoded in sin/cos based , days encoded in sin/cos based)\n",
    "win     = 1024 # Choseen length of slicing window size\n",
    "d_model = 64   # Inner dimension of the model\n",
    "\n",
    "TransAppInstance = get_model_inst(m=m, win=win, dim_model=d_model, mode=\"pretraining\") # Pretraining mode of our TransApp model\n",
    "\n",
    "summary(TransAppInstance, input_size=(1, m, win), mode=\"train\", device='cpu') # show TransApp architecture with pretraining head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pretraining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pretraining = CER_get_data_pretraining(exo_variable=['hours_cos', 'hours_sin', 'days_cos', 'days_sin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrainer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n"
     ]
    }
   ],
   "source": [
    "dict_params = {'lr': 1e-4, 'wd': 1e-4, 'batch_size': 16, 'epochs': 2}\n",
    "save_path = str(root) + '/tmp/TransAppPT' # Model save path\n",
    "\n",
    "pretraining_dataset = TSDataset(data_pretraining, scaler=True, scale_dim=[0])\n",
    "train_loader = torch.utils.data.DataLoader(pretraining_dataset, batch_size=dict_params['batch_size'], shuffle=True)\n",
    "\n",
    "GeomMask = GeometricMask(mean_length=24, masking_ratio=0.5, type_corrupt='zero', dim_masked=0) # Mask to corrupt inout time series\n",
    "\n",
    "model_pretrainer = self_pretrainer(TransAppInstance,                                     \n",
    "                                   train_loader, valid_loader=None,\n",
    "                                   learning_rate=dict_params['lr'], weight_decay=dict_params['wd'],\n",
    "                                   name_scheduler='CosineAnnealingLR',\n",
    "                                   dict_params_scheduler={'T_max': dict_params['epochs'], 'eta_min': 1e-6},\n",
    "                                   warmup_duration=None,\n",
    "                                   criterion=MaskedMSELoss(type_loss='L1'), mask=GeomMask,\n",
    "                                   device=\"cuda\", all_gpu=False,\n",
    "                                   verbose=True, plotloss=True, \n",
    "                                   save_fig=False, path_fig=None,\n",
    "                                   save_only_core=False,\n",
    "                                   save_checkpoint=True, path_checkpoint=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      "    Train loss : 0.462713\n",
      "Adjusting learning rate of group 0 to 5.0500e-05.\n",
      "Epoch [2/2]\n",
      "    Train loss : 0.458446\n",
      "Adjusting learning rate of group 0 to 1.0000e-06.\n"
     ]
    }
   ],
   "source": [
    "model_pretrainer.train(dict_params['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the pretrained model for Appliance Detection (i.e., a chosen classification case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "TransApp                                                [1, 2]                    65\n",
       "├─Sequential: 1-1                                       [1, 1024, 64]             --\n",
       "│    └─DilatedBlock: 2-1                                [1, 64, 1024]             --\n",
       "│    │    └─Sequential: 3-1                             [1, 64, 1024]             64,192\n",
       "│    └─Transpose: 2-2                                   [1, 1024, 64]             --\n",
       "├─Sequential: 1-2                                       [1, 1024, 64]             --\n",
       "│    └─EncoderLayer: 2-3                                [1, 1024, 64]             --\n",
       "│    │    └─LayerNorm: 3-2                              [1, 1024, 64]             128\n",
       "│    │    └─AttentionLayer: 3-3                         [1, 1024, 64]             16,640\n",
       "│    │    └─LayerNorm: 3-4                              [1, 1024, 64]             128\n",
       "│    │    └─PositionWiseFeedForward: 3-5                [1, 1024, 64]             16,576\n",
       "│    │    └─Dropout: 3-6                                [1, 1024, 64]             --\n",
       "│    └─EncoderLayer: 2-4                                [1, 1024, 64]             --\n",
       "│    │    └─LayerNorm: 3-7                              [1, 1024, 64]             128\n",
       "│    │    └─AttentionLayer: 3-8                         [1, 1024, 64]             16,640\n",
       "│    │    └─LayerNorm: 3-9                              [1, 1024, 64]             128\n",
       "│    │    └─PositionWiseFeedForward: 3-10               [1, 1024, 64]             16,576\n",
       "│    │    └─Dropout: 3-11                               [1, 1024, 64]             --\n",
       "│    └─EncoderLayer: 2-5                                [1, 1024, 64]             --\n",
       "│    │    └─LayerNorm: 3-12                             [1, 1024, 64]             128\n",
       "│    │    └─AttentionLayer: 3-13                        [1, 1024, 64]             16,640\n",
       "│    │    └─LayerNorm: 3-14                             [1, 1024, 64]             128\n",
       "│    │    └─PositionWiseFeedForward: 3-15               [1, 1024, 64]             16,576\n",
       "│    │    └─Dropout: 3-16                               [1, 1024, 64]             --\n",
       "│    └─LayerNorm: 2-6                                   [1, 1024, 64]             128\n",
       "├─Sequential: 1-3                                       [1, 2]                    --\n",
       "│    └─Transpose: 2-7                                   [1, 64, 1024]             --\n",
       "│    └─AdaptiveAvgPool1d: 2-8                           [1, 64, 1]                --\n",
       "│    └─Flatten: 2-9                                     [1, 64]                   --\n",
       "│    └─Linear: 2-10                                     [1, 2]                    130\n",
       "│    └─Dropout: 2-11                                    [1, 2]                    --\n",
       "=========================================================================================================\n",
       "Total params: 164,931\n",
       "Trainable params: 164,931\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 65.31\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 19.40\n",
       "Params size (MB): 0.66\n",
       "Estimated Total Size (MB): 20.08\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TransAppInstance.mode = \"classif\" # Change the mode of the TransApp architecture, i.e. use a classification head\n",
    "# OR\n",
    "TransAppInstance = get_model_inst(m=m, win=win, dim_model=d_model, mode=\"classif\", path_select_core= str(root) + '/tmp/TransAppPT.pt') # Load previous pretrained instance\n",
    "\n",
    "summary(TransAppInstance, input_size=(1, m, win), mode=\"train\", device='cpu') # show TransApp architecture with classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a possible detection case on CER dataset\n",
    "\n",
    "- cooker_case\n",
    "- dishwasher_case\n",
    "- waterheater_case\n",
    "- pluginheater_case\n",
    "- tumbledryer_case\n",
    "- tv_greater21inch_case\n",
    "- tv_lessr21inch_case\n",
    "- desktopcomputer_case\n",
    "- laptopcomputer_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 'cooker_case' # exemple of detecting cooker in consumption series\n",
    "\n",
    "datas_tuple = CER_get_data_case('cooker_case', seed=0, exo_variable=['hours_cos', 'hours_sin', 'days_cos', 'days_sin'], win=win)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD Framework instance and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {'lr': 1e-4, 'wd': 1e-3, 'batch_size': 16, 'epochs': 2, 'p_es': 5, 'p_rlr': 3, 'n_warmup_epochs': 0}\n",
    "save_path = str(root) + '/tmp/TransAppPTFinetuned'\n",
    "\n",
    "# Scliced data for training\n",
    "X_train = datas_tuple[0]\n",
    "y_train = datas_tuple[1]\n",
    "X_valid = datas_tuple[2]\n",
    "y_valid = datas_tuple[3]\n",
    "X_test  = datas_tuple[4]\n",
    "y_test  = datas_tuple[5]\n",
    "\n",
    "# Entire curves data for evaluate the model\n",
    "X_train_voter = datas_tuple[6]\n",
    "y_train_voter = datas_tuple[7]\n",
    "X_valid_voter = datas_tuple[8]\n",
    "y_valid_voter = datas_tuple[9]\n",
    "X_test_voter  = datas_tuple[10]\n",
    "y_test_voter  = datas_tuple[11]\n",
    "\n",
    "# Dataset\n",
    "train_dataset = TSDataset(X_train, y_train, scaler=True, scale_dim=[0])\n",
    "valid_dataset = TSDataset(X_valid, y_valid, scaler=True, scale_dim=[0])\n",
    "test_dataset  = TSDataset(X_test,  y_test,   scaler=True, scale_dim=[0])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=dict_params['batch_size'], shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# AD Framework trainer\n",
    "model_trainer = AD_Framework(TransAppInstance,\n",
    "                             train_loader=train_loader, valid_loader=valid_loader,\n",
    "                             learning_rate=dict_params['lr'], weight_decay=dict_params['wd'],\n",
    "                             criterion=nn.CrossEntropyLoss(),\n",
    "                             patience_es=dict_params['p_es'], patience_rlr=dict_params['p_rlr'],\n",
    "                             f_metrics=getmetrics(),\n",
    "                             n_warmup_epochs=dict_params['n_warmup_epochs'],\n",
    "                             scale_by_subseq_in_voter=True, scale_dim=[0],\n",
    "                             verbose=True, plotloss=True, \n",
    "                             save_fig=False, path_fig=None,\n",
    "                             device=\"cuda\", all_gpu=False,\n",
    "                             save_checkpoint=True, path_checkpoint=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      "    Train loss : 0.5554, Train acc : 72.21%\n",
      "    Valid  loss : 0.5587, Valid  acc : 73.83%\n",
      "Epoch [2/2]\n",
      "    Train loss : 0.5385, Train acc : 73.44%\n",
      "    Valid  loss : 0.5823, Valid  acc : 72.88%\n"
     ]
    }
   ],
   "source": [
    "model_trainer.train(dict_params['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored best model met during training.\n",
      "{'ACCURACY': 0.7589670014347202, 'PRECISION': 0.5026178010471204, 'RECALL': 0.5680473372781065, 'PRECISION_MACRO': 0.6791745131717815, 'RECALL_MACRO': 0.694061547426932, 'F1_SCORE': 0.5333333333333334, 'F1_SCORE_MACRO': 0.6854287556415217, 'F1_SCORE_WEIGHTED': 0.763767717777303, 'CONFUSION_MATRIX': array([[ 96,  73],\n",
      "       [ 95, 433]]), 'ROC_AUC_SCORE': 0.7570602474448629, 'ROC_AUC_SCORE_MACRO': 0.7570602474448629, 'ROC_AUC_SCORE_WEIGHTED': 0.7570602474448629}\n"
     ]
    }
   ],
   "source": [
    "#============ eval last model on subsequences ============#\n",
    "model_trainer.evaluate(torch.utils.data.DataLoader(test_dataset, batch_size=1), mask='test_metrics_lastmodel')\n",
    "\n",
    "#============ restore best weight ============#    \n",
    "model_trainer.restore_best_weights()\n",
    "\n",
    "#============ eval model on subsequences  ============#   \n",
    "model_trainer.evaluate(torch.utils.data.DataLoader(test_dataset, batch_size=1))\n",
    "\n",
    "#============ find best quantile on valid voter dataset ============#\n",
    "model_trainer.ADFFindBestQuantile(TSDataset(X_valid_voter, y_valid_voter), m=m, win=win)\n",
    "\n",
    "#============ evaluate on test voter dataset using best quantile ============#\n",
    "quant_metric = model_trainer.ADFvoter_proba(TSDataset(X_test_voter, y_test_voter), m=m, win=win)\n",
    "print(quant_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch-1.8.1]",
   "language": "python",
   "name": "conda-env-pytorch-1.8.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
